from fastapi import FastAPI
from pydantic import BaseModel
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from vector import retriever

# --- Models for API request and response ---
class AskRequest(BaseModel):
    """Request model expecting a 'question' field."""
    question: str

class AskResponse(BaseModel):
    """Response model returning an 'answer' field."""
    answer: str

# --- Initialize FastAPI app ---
app = FastAPI(
    title="Pizza Restaurant QA API",
    description="API to answer questions about a pizza restaurant based on reviews.",
)

model = OllamaLLM(model="llama3.2")

template = """
You are an expert in answering question about a pizza restaurant

Here are some relevant reviews: {reviews}


Here is the question to answer: {questions}
"""

prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

# --- API Endpoint ---
@app.post("/ask", response_model=AskResponse)
async def ask_question(request: AskRequest):
    """
    Accepts a question, retrieves relevant reviews,
    and returns an answer generated by the LLM.
    """
    question_text = request.question
    print(f"Received question: {question_text}") # Optional: Log received question

    # 1. Retrieve relevant reviews
    reviews = retriever.invoke(question_text)
    print(f"Retrieved reviews: {reviews}") # Optional: Log retrieved reviews

    # 2. Invoke the chain with reviews and the question
    result = chain.invoke({"reviews": reviews, "questions": question_text})
    print(f"Generated answer: {result}") # Optional: Log the result

    # 3. Return the result in the expected response format
    return AskResponse(answer=result)

# --- (Optional) Add a root endpoint for basic check ---
@app.get("/")
async def root():
    return {"message": "Pizza Restaurant QA API is running!"}
